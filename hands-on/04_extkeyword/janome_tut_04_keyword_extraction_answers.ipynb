{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "janome_tut_04_keyword_extraction_answers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "vfVYVaenJ7UZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ハンズオン課題4 解答"
      ]
    },
    {
      "metadata": {
        "id": "whtmfdcVKSug",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4-1 解答例\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1DPvQOgL0Ph1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 0. 準備"
      ]
    },
    {
      "metadata": {
        "id": "Q8kLhAmGxm47",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install janome"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jdi8Q2z6xWEJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1. テキストの読み込みと分割\n",
        "\n",
        "テキストファイル `joel_gtd.txt` を読み込み，改行コード `\\n` で split して `sentences` 変数に格納する。"
      ]
    },
    {
      "metadata": {
        "id": "OWpkOyP4K1KG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# janome-tutorial/hands-on/data/joel_gtd.txt を Google Colab にアップロード\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0KVenzqMLNby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat -n joel_gtd.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Dj_ffKrLd0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('joel_gtd.txt') as f:\n",
        "  data = f.read()\n",
        "  \n",
        "sentences = data.split('\\n')\n",
        "sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "900XmeidLpTU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2. トークナイズ\n",
        "\n",
        "`sentences` の各行を形態素解析して，単語のリストのリストに変換して `texts` 変数に格納する。"
      ]
    },
    {
      "metadata": {
        "id": "S2NP41iyLneN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "t = Tokenizer()\n",
        "texts = [t.tokenize(s, wakati=True) for s in sentences]\n",
        "texts[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9PGDbmoMIsQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3. Dictionary 作成\n",
        "\n",
        "`texts` から，gensim の Dictionary を構築して `dictionary` 変数に格納する。\n",
        "\n",
        "参考\n",
        "\n",
        "-  gensim チュートリアル: https://radimrehurek.com/gensim/tut1.html\n"
      ]
    },
    {
      "metadata": {
        "id": "_nP_saHhMV2r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "dictionary.token2id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_x0cxF6eOot_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 4. BoW 化と Corpus 作成\n",
        "\n",
        "構築した `dictionary` を使って，`texts` を文書ベクトル (Bag of Words) に変換し，MmCorpus としてシリアライズする。"
      ]
    },
    {
      "metadata": {
        "id": "QjsbHWEUOuei",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "corpora.MmCorpus.serialize('joel_gtd.mm', corpus)\n",
        "\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t-TPoMLUO5as",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 5. 文書ベクトル (BoW) から，出現回数の多い単語を調べる\n",
        "\n",
        "MmCorpus を読み出し(デシリアライズ)，`corpus` 変数に格納する。テキストファイル 9 行目（「たとえば、あなたのチームの〜」）に出現する上位10単語とその出現回数を表示する。\n",
        "\n",
        "- ヒント1: 9行目の文書ベクトルは， `corpus[8]` で取り出せる。\n",
        "- ヒント2: 単語IDから単語（文字列）への変換は， `dictionary.get(id)` メソッドが使える。 "
      ]
    },
    {
      "metadata": {
        "id": "KydsVfKLO_pg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = corpora.MmCorpus('joel_gtd.mm')\n",
        "high_freq_words = sorted(corpus[8], key=lambda t: t[1], reverse=True)[:10]\n",
        "[(dictionary.get(t[0]), t[1]) for t in high_freq_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mOWeHdpHPopY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 6. TFIDF モデル作成と適用\n",
        "\n",
        "TfidfModel を作成して `tfidf` 変数に格納し，モデルを `corpus` に適用する。テキストファイル 9 行目に現れる単語の中で，tfidf 値の大きい上位10単語とその値を表示する。\n",
        "\n",
        "ひとつ前の結果と比べてみましょう。\n",
        "\n",
        "参考\n",
        "\n",
        "- チュートリアル: https://radimrehurek.com/gensim/tut2.html"
      ]
    },
    {
      "metadata": {
        "id": "2HITsDgiPxBe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "high_tfidf_words = sorted(corpus_tfidf[8], key=lambda t: t[1], reverse=True)[:10]\n",
        "[(dictionary.get(t[0]), t[1]) for t in high_tfidf_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zFHOXJas2Gkr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4-2 解答例\n",
        "\n",
        "#### 0. 準備"
      ]
    },
    {
      "metadata": {
        "id": "DHmCvrDx2R48",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -q http://gensen.dl.itc.u-tokyo.ac.jp/soft/pytermextract-0_01.zip\n",
        "!unzip -oq pytermextract-0_01.zip\n",
        "!cd pytermextract-0_01 && python setup.py install --quiet\n",
        "!pip freeze | grep termextract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g2pvW8Fg41bg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1. テキストファイルのアップロード"
      ]
    },
    {
      "metadata": {
        "id": "k4hHt8Rx44l3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rBuD1sTk4DTU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2. 複合名詞抽出＋出現回数カウント"
      ]
    },
    {
      "metadata": {
        "id": "yGU4u5vH4KCd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "from janome.analyzer import Analyzer\n",
        "from janome.charfilter import *\n",
        "from janome.tokenfilter import *\n",
        "\n",
        "\n",
        "def wc(file):\n",
        "    with open(file, encoding='utf8') as f:\n",
        "        token_filters = [CompoundNounFilter(), POSKeepFilter('名詞,複合'), TokenCountFilter(sorted=True)]\n",
        "        a = Analyzer(tokenizer=Tokenizer(), token_filters=token_filters)\n",
        "        text = f.read()\n",
        "        return list(a.analyze(text))\n",
        "      \n",
        "\n",
        "counts = wc('article.txt')\n",
        "for k, v in counts:\n",
        "  print('%s\\t%d' % (k, v))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}