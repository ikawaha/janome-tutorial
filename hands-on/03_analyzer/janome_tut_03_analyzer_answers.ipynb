{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "janome_tut_03_analyzer_answers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Jm_hP69g79sF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ハンズオン課題3 解答\n",
        "\n",
        "**実行するには，メニューの「PLAYGROUNDで開く」を押してください。（Google アカウントが必要です。）**"
      ]
    },
    {
      "metadata": {
        "id": "AUivoSTF755V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# janome をインストール\n",
        "!pip install janome\n",
        "\n",
        "# https://ipafont.ipa.go.jp/old/ipafont/download.html から TTFファイル「IPA P ゴシック」をダウンロードして手元で解凍し，`ipagp.ttf` ファイルを Google Colab にアップロード\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B8CGIHwt90Y5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3-1 解答例"
      ]
    },
    {
      "metadata": {
        "id": "lLzCZan9-EQp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# janome-tutorial/hands-on/data/kazeno_matasaburo_utf8.txt を Google Colab にアップロード\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kfxEN5D695Oe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "from janome.analyzer import Analyzer\n",
        "from janome.charfilter import *\n",
        "from janome.tokenfilter import *\n",
        "import sys\n",
        "\n",
        "MAX_WORDS = 20\n",
        "\n",
        "def wc(file):\n",
        "    with open(file, encoding='utf8') as f:\n",
        "        \"\"\"\n",
        "        ここに処理を書く。\n",
        "        戻り値として，(単語, 出現回数) のタプルのリストを返す。\n",
        "        \"\"\"\n",
        "        token_filters = [TokenCountFilter(sorted=True)]\n",
        "        a = Analyzer(tokenizer=Tokenizer(), token_filters=token_filters)\n",
        "        text = f.read()\n",
        "        return list(a.analyze(text))[:MAX_WORDS]\n",
        "      \n",
        "counts = wc('kazeno_matasaburo_utf8.txt')\n",
        "for k, v in counts:\n",
        "    print('%s\\t%d' % (k, v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wMid1sXQ_yO5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3-2 解答例"
      ]
    },
    {
      "metadata": {
        "id": "fbKQ0W0Q_1cO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "from janome.analyzer import Analyzer\n",
        "from janome.charfilter import *\n",
        "from janome.tokenfilter import *\n",
        "\n",
        "MAX_WORDS = 20\n",
        "\n",
        "def wc(file, pos=[]):\n",
        "    with open(file, encoding='utf8') as f:\n",
        "        if not pos:\n",
        "            token_filters = [TokenCountFilter(sorted=True)]\n",
        "        else:\n",
        "            token_filters = [POSKeepFilter(pos), TokenCountFilter(sorted=True,att='base_form')]\n",
        "        a = Analyzer(tokenizer=Tokenizer(), token_filters=token_filters)\n",
        "        text = f.read()\n",
        "        return list(a.analyze(text))[:MAX_WORDS]\n",
        "\n",
        "# 名詞と動詞に絞ってカウント\n",
        "counts = wc('kazeno_matasaburo_utf8.txt', ['名詞','動詞'])\n",
        "for k, v in counts:\n",
        "    print('%s\\t%d' % (k, v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vyBxpPl6Acmo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3-3 解答例"
      ]
    },
    {
      "metadata": {
        "id": "eipe2IdXCqYn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# StopWordFilter の実装\n",
        "from janome.tokenizer import Tokenizer\n",
        "from janome.analyzer import Analyzer\n",
        "from janome.charfilter import *\n",
        "from janome.tokenfilter import *\n",
        "\n",
        "class StopWordFilter(TokenFilter):\n",
        "  def __init__(self, word_list=[], word_list_file=''):\n",
        "    self.word_list = []\n",
        "    if word_list:\n",
        "      self.word_list = word_list\n",
        "    elif word_list_file:\n",
        "      with open(word_list_file) as f:\n",
        "        for line in f:\n",
        "          word = line.strip()\n",
        "          self.word_list.append(word)\n",
        "    \n",
        "  def apply(self, tokens):\n",
        "      for token in tokens:\n",
        "        if any(token.base_form == word for word in self.word_list):\n",
        "          continue\n",
        "        yield token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G0VE6YZDFRPM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# stop word をリストで指定する\n",
        "token_filters = [StopWordFilter(word_list=['プログラミング'])]\n",
        "a = Analyzer(tokenizer=Tokenizer(), token_filters=token_filters)\n",
        "for token in a.analyze('Pythonは人気の高いプログラミング言語です。'):\n",
        "  print(token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xkR99n8yFEJ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# stop word をファイルで指定する\n",
        "!echo \"プログラミング\" > stop_words.txt\n",
        "!echo \"Python\" >> stop_words.txt\n",
        "\n",
        "token_filters = [StopWordFilter(word_list_file='stop_words.txt')]\n",
        "a = Analyzer(tokenizer=Tokenizer(), token_filters=token_filters)\n",
        "for token in a.analyze('Pythonは人気の高いプログラミング言語です。'):\n",
        "  print(token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8CwMLlm1HY_3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3-4 解答例"
      ]
    },
    {
      "metadata": {
        "id": "JUixTLeFHbf3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!echo 又三郎,名詞-カスタム,マタサブロー > 'udic.csv'\n",
        "\n",
        "from janome.tokenizer import Tokenizer\n",
        "from janome.analyzer import Analyzer\n",
        "from janome.charfilter import *\n",
        "from janome.tokenfilter import *\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_analyzer(udic_file, udic_type):\n",
        "    tokenizer = Tokenizer(udic_file, udic_type=udic_type, udic_enc='utf8') if udic_file else Tokenizer()\n",
        "    char_filters = [RegexReplaceCharFilter('《.*?》', '')]\n",
        "    token_filters = [POSKeepFilter(['名詞','形容詞','形容動詞','感動詞']), POSStopFilter(['名詞,非自立','名詞,代名詞']), ExtractAttributeFilter('base_form')]\n",
        "    return Analyzer(char_filters, tokenizer, token_filters)\n",
        "\n",
        "def split_text(src, out, udic_file='udic.csv', udic_type='simpledic'):\n",
        "    \"\"\"\n",
        "    src で渡されたファイルを読み，単語分割して out に書き出します。\n",
        "    \"\"\"\n",
        "    a = create_analyzer(udic_file, udic_type)\n",
        "    with open(src, encoding='utf8') as f1:\n",
        "        with open(out, mode='w', encoding='utf8') as f2:\n",
        "            for line in f1:\n",
        "                tokens = list(a.analyze(line))\n",
        "                f2.write('%s\\n' % ' '.join(tokens))\n",
        "\n",
        "def show_wordcloud(file):\n",
        "    with open(file, encoding='utf8') as f:\n",
        "        text = f.read()\n",
        "        wordcloud = WordCloud(font_path='ipagp.ttf', background_color='white', width=1024, height=674).generate(text)\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.figure()\n",
        "        plt.show()\n",
        "        \n",
        "split_text('kazeno_matasaburo_utf8.txt', 'words.txt')\n",
        "show_wordcloud('words.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}